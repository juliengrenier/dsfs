{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./dsfs/ml.py\n",
    "import random\n",
    "\n",
    "from typing import TypeVar, List, Tuple\n",
    "\n",
    "X = TypeVar('X')\n",
    "Y = TypeVar('Y')\n",
    "\n",
    "\n",
    "def split_data(data: List[X], prob: float, shuffle: bool = True) -> Tuple[List[X], List[X]]:\n",
    "    data = data[:]\n",
    "    if shuffle: random.shuffle(data)\n",
    "\n",
    "    cut = int(len(data) * prob)\n",
    "    return data[:cut], data[cut:]\n",
    "\n",
    "\n",
    "def train_test_split(xs: List[X], ys: List[Y], test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    idx = [i for i in range(len(xs))]\n",
    "    train_idx, test_idx = split_data(idx, 1 - test_pct)\n",
    "    return (\n",
    "        [xs[i] for i in train_idx],\n",
    "        [xs[i] for i in test_idx],\n",
    "        [ys[i] for i in train_idx],\n",
    "        [ys[i] for i in test_idx]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(range(1000))\n",
    "train, test = split_data(data, 0.75)\n",
    "assert len(train) == 750\n",
    "assert len(test) == 250\n",
    "assert sorted(train + test) == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(1000))\n",
    "ys = [2 * x for x in xs]\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(y_test) == len(x_test) == 250\n",
    "\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))\n",
    "assert all(y == 2 * x for x, y in zip(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./dsfs/scoring.py\n",
    "\n",
    "# tp -> true positive\n",
    "# fp -> false positive\n",
    "# fn -> false negative\n",
    "# tn -> true negative\n",
    "\n",
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    \"The proportion of correct predictions\"\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    \"Precision measures the accuracy of the positive predictions\"\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    \"Recall measures the proportion of the positives identified\"\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tp)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p + r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Leukemia | No Leukemia | Total   |\n",
    "|------------|----------|-------------|---------|\n",
    "| \"Luke\"     | 70       |      4930   |  5000   |\n",
    "| Not \"Luke\" | 13930    |      981070 |  995000 |\n",
    "| total      | 14000    |      986000 | 1000000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00736842105263158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn, tn = 70, 4930, 13930, 981070\n",
    "\n",
    "assert accuracy(tp, fp, fn, tn) == 0.98114, accuracy(tp, fp, fn, tn) # High Accuracy\n",
    "assert precision(tp, fp, fn, tn) == 0.014, precision(tp, fp, fn, tn) # Low precision\n",
    "assert recall(tp, fp, fn, tn) == 0.005, recall(tp, fp, fn, tn) # Low recall\n",
    "\n",
    "f1_score(tp, fp, fn, tn) # Really bad f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
